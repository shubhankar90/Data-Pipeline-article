{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an ELT data pipeline for combining data from multiple sources\n",
    "\n",
    "In this article I will be describing a tutorial for creating a data pipeline using free tools. This is meant to serve as a high level architecture guide. In particular this will be useful when you want to **EXTRACT** data from multiple data sources, **LOAD** it to your local system and **TRANSFORM** it for your database schema. This will be an [ELT](https://en.wikipedia.org/wiki/Extract,_load,_transform) process. ETL tools cost a ton to buy and use. It doesn't make sense to make the purchase for small to medium size projects. I have used FREE softwares for this post.\n",
    "\n",
    "What you will need:\n",
    "1. Python for all your data extraction requirements. You can use Python for web-scraping, querying API's, pulling data from csv files or emails.\n",
    "2. Understanding of cron jobs or windows scheduler for triggering python code for data pull and data transformation (Hint for windows: Create .bat files with the batch code to run the python code).\n",
    "3. A database where you would keep your transformed data in an appropriate schema. I will suggest using the database as your data transformation workhorse if the transformations are possible in SQL. You can use MySQL running on a windows system if the data is not expected to grow too big for a single system to handle the load.\n",
    "4. And a desktop (obviously) which can be left turned ON for a long time, If you need to update your data very frequently.\n",
    "\n",
    "Overall Architecture:\n",
    "![img](https://raw.githubusercontent.com/shubhankar90/Data-Pipeline-article/master/Overall_flow.png)\n",
    "\n",
    "## Details regarding extract, load, transform steps\n",
    "- Create a database\\datamart schema which fits your analytics requirements. I won't go into datails of database schema design as it is a sufficiently big topic in itself.\n",
    "- Create staging tables in your database which will act as temporary data storage areas. You need not check for data consistencies or errors before loading data here. These need to be emptied after each ETL cycle. You can keep collecting the raw data here and consider them as your historical backup store, but this will increase the complexity of your transformation code.\n",
    "- You should create separate historical tables which will store the raw data pulled from the data sources.\n",
    "- **Extract**\n",
    "    - Use python or any other programming language for extracting data from sources. Do remember to code in the logic for checking the last updated date to download data for that time onwards. You don't want to download data which is already in your database schema. This code will also load the downloaded data into your database staging tables.\n",
    "- **Load**\n",
    "    - Load the raw data into historical tables and staging table.\n",
    "- **Transform**\n",
    "    - Create SQL code to check for error or data inconsistencies in the staging tables. Load incononsistent rows into error tables.\n",
    "    - Create SQL code to transform the raw data to the required database schema and load it into your database schema.\n",
    "    - If the transformation or data error finding is complex then use python to transform data and load it again into your database schema.\n",
    "    - Delete data from staging tables.\n",
    "    - This step shoudld be triggered by the ending of the extraction python script. You can use python script to trigger transforation SQL.\n",
    "\n",
    "I hope this gives you a overview of how to design your data pipeline. Do let me know in the comments if you want further details regarding any step.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
